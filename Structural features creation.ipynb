{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, os.path, re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize, scale\n",
    "from collections import Counter\n",
    "from sys import getsizeof\n",
    "import time\n",
    "from sklearn.cluster import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data preprocessing **\n",
    "\n",
    "Extract filenames and reorder it numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_path = 'collected_models/'\n",
    "ts_labels = ['heart_rate', 'oxygen_concentration', 'chest_volume']\n",
    "\n",
    "number_ts_pieces = len(os.listdir(files_path)) / len(ts_labels)\n",
    "# if the number is not divided \n",
    "if abs(number_ts_pieces - round(number_ts_pieces)) > 0:\n",
    "    print('ERROR: invalid number of files in ', files_path)\n",
    "else:\n",
    "    number_ts_pieces = int(number_ts_pieces)    \n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text):    \n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "file_names = sorted(os.listdir(files_path), key=natural_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Extract models from files **\n",
    "\n",
    "After the extraction we preprocess them. Eliminate parameters and refactor the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_models_from_file(number_of_file, type_of_ts):\n",
    "    index_of_ts = ts_labels.index(type_of_ts)\n",
    "    file_name = file_names[number_ts_pieces * index_of_ts + number_of_file]        \n",
    "    file_opened = open(files_path + file_name, 'r')\n",
    "    func_and_params_names = file_opened.readlines()\n",
    "    models = []\n",
    "    for entity in func_and_params_names:          \n",
    "        entity = entity.split(' ')[-1]\n",
    "        models.append(entity)\n",
    "        models[-1] = models[-1].strip()                    \n",
    "        models[-1] = re.sub(r'X\\[(\\d+)\\]', r'x\\1', models[-1])    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write functions responsible for conversion of a string-handle to a tree matrix and codings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_number_of_tokens(handle):    \n",
    "    counter_tokens = 0\n",
    "    counter_variables = 0\n",
    "\n",
    "    for i in range(len(handle)):\n",
    "        if handle[i] == '_': \n",
    "            counter_tokens += 1\n",
    "        elif i < len(handle)-1 and handle[i] == 'x' and handle[i+1].isdigit():\n",
    "            counter_variables += 1;\n",
    "\n",
    "    return (counter_tokens, counter_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_map_tokens_params():\n",
    "    file_opened = open('data/numbParam.txt', 'r')\n",
    "    primitives_lines = file_opened.readlines()\n",
    "    tokens_codes = {line.split()[0] : int(ind) for ind,line in enumerate(primitives_lines)}    \n",
    "    tokens_params = {line.split()[0] : int(line.split()[1]) for line in primitives_lines}    \n",
    "    return (tokens_codes, tokens_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dfs_search_on_handle(handle):\n",
    "    counters = find_number_of_tokens(handle)\n",
    "    number_tokens = counters[0] + counters[1]\n",
    "    \n",
    "    waiting_tokens = []\n",
    "    encodings = np.zeros(number_tokens, dtype = int)        \n",
    "    current_token, left, right = 0, 0, 0\n",
    "    is_a_token_processed_now = False    \n",
    "    \n",
    "    map_tokens_params = create_map_tokens_params()[0]\n",
    "    \n",
    "    for right in range(len(handle)):\n",
    "        if handle[right] == '_':\n",
    "            # the root is detected\n",
    "            waiting_tokens.append(current_token)\n",
    "            token = handle[left:right + 1]\n",
    "            encodings[current_token] = map_tokens_params[token]\n",
    "            right += 1\n",
    "            break;  \n",
    "    \n",
    "    matr = [[] for i in range(number_tokens)]            \n",
    "    \n",
    "    # now process the remaining vertices\n",
    "    reserved_right = right\n",
    "    for right in np.arange(right, len(handle)):\n",
    "        if handle[right] == ')':\n",
    "            waiting_tokens.pop()        \n",
    "    \n",
    "        if not is_a_token_processed_now and handle[right].isalpha():\n",
    "            is_a_token_processed_now = True\n",
    "            left = right\n",
    "    \n",
    "        # if a token is found\n",
    "        if handle[right] == '_':\n",
    "            # new token is detected\n",
    "            current_token += 1\n",
    "            matr[waiting_tokens[-1]].append(current_token)\n",
    "            waiting_tokens.append(current_token)\n",
    "            token = handle[left:right + 1]\n",
    "            encodings[current_token] = map_tokens_params[token]\n",
    "            is_a_token_processed_now = False      \n",
    "        \n",
    "        # if a variable is found\n",
    "        if right < len(handle)-1 and handle[right] == 'x' and handle[right+1].isdigit():\n",
    "            # new variable is detected\n",
    "            current_token += 1\n",
    "            matr[waiting_tokens[-1]].append(current_token)\n",
    "            while right < len(handle)-1 and handle[right] == 'x' and handle[right+1].isdigit():\n",
    "                right += 1\n",
    "            token = handle[left:right + 1]\n",
    "            encodings[current_token] = map_tokens_params[token]\n",
    "            is_a_token_processed_now = False            \n",
    "    \n",
    "    return (matr, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def incidence_to_adjacency(incidence):\n",
    "    size_of_mat = len(incidence[0])\n",
    "    adjacency = np.zeros((size_of_mat, size_of_mat))    \n",
    "    for ind, row in enumerate(incidence[0]):\n",
    "        adjacency[ind][row] = 1\n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Collect primitive structural features from a population of models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_simple_features_from_ts(number_of_file, type_of_ts, tokens_codes):\n",
    "    models = get_models_from_file(number_of_file, type_of_ts)\n",
    "    \n",
    "    primitive_frequences = np.zeros(len(tokens_codes) + 1)\n",
    "    lower_bound_code_variables = tokens_codes['x0']\n",
    "\n",
    "    for model in models:\n",
    "        matr, encodings = dfs_search_on_handle(model)\n",
    "        model_primitive_frequences = Counter(encodings)\n",
    "        for key in model_primitive_frequences:\n",
    "            primitive_frequences[key] += model_primitive_frequences[key]\n",
    "        primitive_frequences[-1] += len(encodings)\n",
    "    primitive_frequences[-1] = primitive_frequences[-1] / len(models)\n",
    "    #return normalize(primitive_frequences.reshape(-1,1), axis=0)\n",
    "    return primitive_frequences.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_codes, _ = create_map_tokens_params()\n",
    "feature_matrices_of_ts = {label : np.zeros((len(tokens_codes) + 1, number_ts_pieces)) for label in ts_labels}\n",
    "\n",
    "for label in ts_labels:\n",
    "    for index in range(number_ts_pieces):        \n",
    "        feature_matrices_of_ts[label][:,index] = get_simple_features_from_ts(index, label, tokens_codes)[:,0]\n",
    "        \n",
    "#feature_matrices_of_ts = {label : scale(feature_matrices_of_ts[label].T) for label in ts_labels}        \n",
    "feature_matrices_of_ts = {label : feature_matrices_of_ts[label].T for label in ts_labels}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unite_feature_matrix = np.vstack((feature_matrices_of_ts[label] for label in ts_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now perform two-class classification **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "which_label_is_positive = 'oxygen_concentration'\n",
    "\n",
    "target_vector = np.zeros((unite_feature_matrix.shape[0],1))\n",
    "ts_labels_in_order_from_dictionary = [label for label in feature_matrices_of_ts]\n",
    "index_of_label_positive = ts_labels_in_order_from_dictionary.index(which_label_is_positive)\n",
    "all_inidices_of_samples = np.arange(target_vector.shape[0])\n",
    "positive_indices_samples = number_ts_pieces * index_of_label_positive + np.arange(number_ts_pieces)\n",
    "negative_positive_samples = [ind for ind in all_inidices_of_samples if not ind in positive_indices_samples]\n",
    "target_vector[number_ts_pieces * index_of_label_positive:number_ts_pieces * (index_of_label_positive + 1)] = np.ones((number_ts_pieces,1))\n",
    "backup_target = target_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide set of samples on training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fraction_of_test_samples = 0.3\n",
    "\n",
    "indices_of_test_sample = np.random.choice([True, False], len(all_inidices_of_samples), p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "target_vector = 2* target_vector - 1 \n",
    "train_matrix = unite_feature_matrix[~indices_of_test_sample,:]\n",
    "test_matrix = unite_feature_matrix[indices_of_test_sample,:]\n",
    "train_target = target_vector[~indices_of_test_sample].reshape(sum(~indices_of_test_sample),)\n",
    "test_target = target_vector[indices_of_test_sample].reshape(sum(indices_of_test_sample),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 37)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-45deeb1847fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_support_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    405\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 407\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 37)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_matrix, train_target) \n",
    "print(clf.n_support_)\n",
    "predictions = clf.predict(test_matrix)\n",
    "print(\"predictions: \", predictions)\n",
    "errors = predictions != test_target\n",
    "print(\"error = \", sum(errors) / len(errors))\n",
    "print(\"error on positive = \", sum(errors[test_target == max(test_target)]) / len(errors[test_target == max(test_target)]))\n",
    "print(\"error on negative = \", sum(errors[test_target == min(test_target)]) / len(errors[test_target == min(test_target)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40 33]\n",
      "predictions:  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "error =  0.314285714286\n",
      "error on positive =  1.0\n",
      "error on negative =  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_matrix, train_target) \n",
    "print(clf.n_support_)\n",
    "predictions = clf.predict(train_matrix)\n",
    "print(\"predictions: \", predictions)\n",
    "errors = predictions != train_target\n",
    "print(\"error = \", sum(errors) / len(errors))\n",
    "print(\"error on positive = \", sum(errors[train_target == max(train_target)]) / len(errors[train_target == max(train_target)]))\n",
    "print(\"error on negative = \", sum(errors[train_target == min(train_target)]) / len(errors[train_target == min(train_target)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  uniform\n",
      "error =  0.326923076923\n",
      "error on positive =  0.722222222222\n",
      "error on negative =  0.117647058824\n",
      "weights:  distance\n",
      "error =  0.326923076923\n",
      "error on positive =  0.722222222222\n",
      "error on negative =  0.117647058824\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "\n",
    "\n",
    "n_neighbors = 1\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(train_matrix, train_target)\n",
    "\n",
    "    predictions = clf.predict(np.c_[test_matrix])\n",
    "    print(\"weights: \", weights)\n",
    "    errors = predictions != test_target\n",
    "    print(\"error = \", sum(errors) / len(errors))\n",
    "    print(\"error on positive = \", sum(errors[test_target == max(test_target)]) / len(errors[test_target == max(test_target)]))\n",
    "    print(\"error on negative = \", sum(errors[test_target == min(test_target)]) / len(errors[test_target == min(test_target)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.32692307692307693)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "error = np.inf\n",
    "where_min = -1\n",
    "\n",
    "for n_neighbors in 1 + np.arange(30):\n",
    "    for weights in ['uniform', 'distance']:\n",
    "        # we create an instance of Neighbours Classifier and fit the data.\n",
    "        clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "        clf.fit(train_matrix, train_target)\n",
    "\n",
    "        predictions = clf.predict(np.c_[test_matrix])\n",
    "        errors = predictions != test_target\n",
    "        if sum(errors) / len(errors) < error:\n",
    "            where_min = n_neighbors\n",
    "            error = sum(errors) / len(errors)\n",
    "            \n",
    "\n",
    "where_min, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Collect DFS-code features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def levenstein_string_distance(a, b):\n",
    "    n, m = len(a), len(b)\n",
    "    if n > m:\n",
    "        # Make sure n <= m, to use O(min(n,m)) space\n",
    "        a, b = b, a\n",
    "        n, m = m, n\n",
    "\n",
    "    current_row = range(n+1) # Keep current and previous row, not entire matrix\n",
    "    for i in range(1, m+1):\n",
    "        previous_row, current_row = current_row, [i]+[0]*n\n",
    "        for j in range(1,n+1):\n",
    "            add, delete, change = previous_row[j]+1, current_row[j-1]+1, previous_row[j-1]\n",
    "            if a[j-1] != b[i-1]:\n",
    "                change += 1\n",
    "            current_row[j] = min(add, delete, change)\n",
    "\n",
    "    return current_row[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distance_between_populations(population_first, population_second):\n",
    "    len_first, len_second = len(population_first), len(population_second)\n",
    "    # calculate sum of distances for all pairs of models: one from first population, the other from second\n",
    "    cumulative_distance = 0\n",
    "    for model_from_first in population_first:\n",
    "        for model_from_second in population_second:\n",
    "            cumulative_distance = cumulative_distance + levenstein_string_distance(model_from_first, model_from_second)\n",
    "    cumulative_distance = cumulative_distance / (len_first * len_second)\n",
    "    return cumulative_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_simple_features_from_ts(number_of_file, type_of_ts, tokens_codes):\n",
    "    models = get_models_from_file(number_of_file, type_of_ts)\n",
    "    \n",
    "    primitive_frequences = np.zeros(len(tokens_codes))    \n",
    "\n",
    "    for model in models:\n",
    "        matr, encodings = dfs_search_on_handle(model)\n",
    "        primitive_frequences = encodings\n",
    "\n",
    "    return primitive_frequences.reshape(-1,1)\n",
    "    #return primitive_frequences.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_matrix_of_codes_of_one_population(number_of_file, type_of_ts, tokens_codes):\n",
    "    codes = 'QWERTYUIOPASDFGHJKLZXCVBNM123456789/*-+=?!'\n",
    "    models = get_models_from_file(number_of_file, type_of_ts)\n",
    "    \n",
    "    matrix_representation = []    \n",
    "    for model in models:\n",
    "        matr, encodings = dfs_search_on_handle(model)\n",
    "        encodings = np.array(encodings)\n",
    "        if len(model) == 0:\n",
    "            break\n",
    "        matrix_representation.append(''.join(np.array(list(codes))[encodings]))\n",
    "\n",
    "    return matrix_representation\n",
    "    #return primitive_frequences.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_codes, _ = create_map_tokens_params()\n",
    "feature_matrices_of_ts = {label : [] for label in ts_labels}\n",
    "    \n",
    "for label in ts_labels:\n",
    "    for index in range(number_ts_pieces):\n",
    "        feature_matrices_of_ts[label].append(create_matrix_of_codes_of_one_population(index, label, tokens_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart_rate heart_rate\n",
      "heart_rate oxygen_concentration\n",
      "heart_rate chest_volume\n",
      "oxygen_concentration heart_rate\n",
      "oxygen_concentration oxygen_concentration\n",
      "oxygen_concentration chest_volume\n",
      "chest_volume heart_rate\n",
      "chest_volume oxygen_concentration\n",
      "chest_volume chest_volume\n",
      "400.93179059028625\n"
     ]
    }
   ],
   "source": [
    "distances_between_segments = np.zeros((len(ts_labels) * number_ts_pieces, len(ts_labels) * number_ts_pieces))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for ind_label_f,label_f in enumerate(feature_matrices_of_ts):\n",
    "    for ind_label_s,label_s in enumerate(feature_matrices_of_ts):\n",
    "        print(label_f, label_s)\n",
    "        for ind_population_f,population_f in enumerate(feature_matrices_of_ts[label_f]):\n",
    "            for ind_population_s,population_s in enumerate(feature_matrices_of_ts[label_s]):\n",
    "                index_first = ind_label_f * number_ts_pieces + ind_population_f\n",
    "                index_second = ind_label_s * number_ts_pieces + ind_population_s\n",
    "                distances_between_segments[index_first][index_second] = distance_between_populations(population_f[0:10], population_s[0:10])\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.566666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0,\n",
       "       2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 1, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 50\n",
    "clusters = KMeans(n_clusters=3, random_state=random_state).fit_predict(distances_between_segments)\n",
    "true_clusters = np.zeros(len(ts_labels) * number_ts_pieces)\n",
    "for ind,_ in enumerate(ts_labels):\n",
    "    true_clusters[ind * number_ts_pieces:(ind + 1) * number_ts_pieces] = ind * np.ones(number_ts_pieces)\n",
    "print(sum(clusters != true_clusters) / len(clusters))\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 170\n",
    "clusters = AgglomerativeClustering(n_clusters=3).fit_predict(distances_between_segments)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [0,2,3,4]\n",
    "l[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraction_of_test_samples = 0.3\n",
    "\n",
    "test_indices = np.random.choice([True, False], number_ts_pieces, p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "target_vector = 2* target_vector - 1 \n",
    "\n",
    "train_matrix = unite_feature_matrix[~indices_of_test_sample,:]\n",
    "test_matrix = unite_feature_matrix[indices_of_test_sample,:]\n",
    "train_target = target_vector[~indices_of_test_sample].reshape(sum(~indices_of_test_sample),)\n",
    "test_target = target_vector[indices_of_test_sample].reshape(sum(indices_of_test_sample),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
