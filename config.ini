[data_extraction]
dataset_filename 	 = /data/data_to_fit.txt
init_models_filename = /data/init_models.txt

[flag_type_of_processing]
# variants: 'fit_data', 'labeled_fit_data', 'time_series_processing', 'fit_and_collect', 'observing_final_model', 'rules_creation',
# 'init_models_creation'
flag = rules_creation

[tokens_info]
info_for_curve_fit = /code/primitives/PrimitivesInfoForOptimization.txt
file_with_primitives = /code/primitives/Primitives.py

[model_generation]
# the number of crossings among the population on a iteration
crossing_number 	 = 5
# the number of mutations among the population on a iteration
mutation_number 	 = 2
# the number of random generated models on a iteration
random_models_number = 2
# the number of best models from a generated population to pass to the next iteration
best_models_number	 = 10

# structural complexity of random models generated
random_models_complexity = 10
# flag signifying if the parameters of superpositions will be tuned
# if 'False' is set, then all parameters (if presented) are set to 1
is_parametric = True

# type of superpositions selection
# variants: [MSE, Error_structural, Penalize_params]
type_selection       = Error_structural
# penalty on excessive structural complexity (number of tokens in superposition)
# if a model has MSE equal to 'x' and structural complexity equal to 'y', then the additive penalty is 'xy'
# -- the best for fitting models to data is 0.00002
structural_penalty   = 0.01
parameters_penalty   = 0.000000002

# specifies maximum number of parameters
maximum_param_number = 17
# specifies maximum structural complexity of a model
maximum_complexity 	 = 26


# if True, we perform multistart in scipy
multistart = True
iterations_multistart = 10
# are bounds used in scipy curve_fit
bounds_included = False

[init_rand_models]
# if the flag is set to 'False', we do not load bunch of random init models
do_init_random_generation = False
# for better convergence we start generation with big bunch of random models
number_of_init_random_models = 10000
size_init_random_models		 = 6
# preferable number of stored initial models
preferable_number_models = 300
# purpose of creation
# variants = [rules_creation, fitting_ot_data_or_ts]
purpose = rules_creation


#
#
#
[accuracy_requirement]
# maximum number of performed cycles (evolutions)
max_number_cycle_count = 1
# required accuracy; when it is reached, evolutions are stopped
required_accuracy = 0.001


#
#
#
[time_series_processing]
# set of labels of ts to classify
# labels = chest_volume, heart_rate, oxygen_concentration, open_apple
labels = chest_volume, oxygen_concentration
# locations of the ts
root_path = /ts_processing/
# extension of files containing data about ts
extension = .txt
# folder for output files
where_to_store_models = populations/collected_models21/
# specifies number of segments for ts to be split on
number_of_segments = 100	


#
#
#
[stagnation]
# if MSE changes very slow (< threshold_of_slow_mse_change) for the last 'window_size' iterations ...
# ... starting from 'init_iteration', we break the generation
lowest_possible_rate = 0.01
window_size    = 100
init_iteration = 200



#
#
#
[fit_and_collect]
# folder for output files
where_to_store_models = populations/first_segments4/
# extension of files containing data about ts
extension = .txt
# number of times of fitting
number_fittings = 3



#
#
#
[rules_creation]
# folder, in which we store received rules
rules_folder = /data/Rules_creation_files/
rules_filename = received_rules.txt
rules_used_in_fitting = /data/rules.txt

# name of a file containing initial models necessary to rules creation
init_patterns_filename = init_patterns.txt
init_replacements_filename = init_replacements.txt

# range of values of independent variable
range_independent_var = [-1,1]
number_of_samples = 100
number_of_vars = 2

# regime of creation of rules
# variants: [create_patterns, create_replacements]
regime = create_replacements

# if a parameters range contains inf, we replace it with the following constant to perform
# randomization
inf_replace = 1
maximum_size_replacements = 10

# given a pattern/replacement, we launch the GA to find models giving similar values as of the pattern/replacement
# for each pattern/replacement GA will be launched 'iterations_of_fitting' of times
iterations_of_fitting = 1

# when a set of candidates is found, each is checked thoroughly to be equivalent to the original model
iterations_to_check_fitness = 30

# threshold for permissible error of similarity of models from the rule
threshold = 0.04

# when we perform multistart on the parameters of a pattern/replacement we can receive some misfittings due to imperfection of scipy's curve_fit therefore, we set a fraction of permissible misfittings
fraction_of_misfittings = 0.04

# when a model receives random parameters, it could produce enormous values
# to avoid this, we set a maximum possible value used in fittings
maximum_value = 1000 * 1000