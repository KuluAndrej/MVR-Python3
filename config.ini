[data_extraction]
dataset_filename 	 = /data/data_to_fit.txt
init_models_filename = /data/init_models.txt

[flag_type_of_processing]
# variants: 'fit_data', 'time_series_processing', 'fit_and_collect', 'observing_final_model'
flag = time_series_processing

[tokens_info]
info_for_curve_fit = code/PrimitivesInfoForOptimization.txt
file_with_primitives = code/Primitives.py

[model_generation]
# the number of crossings among the population on a iteration
crossing_number 	 = 30
# the number of mutations among the population on a iteration
mutation_number 	 = 30
# the number of random generated models on a iteration
random_models_number = 0
# structural complexity of random models generated
random_models_complexity = 30
# the number of best models from a generated population to pass to the next iteration
best_models_number	 = 35
# flag signifying if the parameters of superpositions will be tuned
# if 'False' is set, then all parameters (if presented) are set to 1
is_parametric = True
# type of superpositions selection
# variants: [MSE, Error_structural, Penalize_params]
type_selection       = Error_structural
# penalty on excessive structural complexity (number of tokens in superposition)
# if a model has MSE equal to 'x' and structural complexity equal to 'y', then the additive penalty is 'xy'
# -- the best for fitting models to data is 0.00002
structural_penalty   = 0.0005
parameters_penalty   = 0.000000002
# specifies maximum number of parameters
maximum_param_number = 14
# specifies maximum structural complexity of a model
maximum_complexity 	 = 22
# for better convergence we start generation with big bunch of random models
number_of_init_random_models = 10000
size_init_random_models		 = 4
# if the flag is set to 'False', we do not load bunch of random init models
do_init_random_generation = False

[accuracy_requirement]
# maximum number of performed cycles (evolutions)
max_number_cycle_count = 10
# required accuracy; when it is reached, evolutions are stopped
required_accuracy = 0.001

[time_series_processing]
# set of labels of ts to classify
# labels = chest_volume, heart_rate, oxygen_concentration, open_apple
labels = heart_rate, open_apple
# locations of the ts
root_path = /ts_processing/
# extension of files containing data about ts
extension = .txt
# folder for output files
where_to_store_models = populations/collected_models20/
# specifies number of segments for ts to be split on
number_of_segments = 100	

[stagnation]
# if MSE changes very slow (< threshhold_of_slow_mse_change) for the last 'window_size' iterations ...
# ... starting from 'init_iteration', we break the generation
lowest_possible_rate = 0.01
window_size    = 100
init_iteration = 200

[fit_and_collect]
# folder for output files
where_to_store_models = populations/first_segments4/
# extension of files containing data about ts
extension = .txt
# number of times of fitting
number_fittings = 3