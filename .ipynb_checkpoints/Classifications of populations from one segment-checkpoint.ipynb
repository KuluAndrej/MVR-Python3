{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import block **\n",
    "==========\n",
    "\n",
    "Take a notice of cells in the Appendix.\n",
    "Some sells below depend on these ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, os.path, re\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import normalize, scale, MinMaxScaler\n",
    "from sklearn.cluster import *\n",
    "\n",
    "from collections import Counter\n",
    "from sys import getsizeof\n",
    "\n",
    "from code.modules.levenshtein import levenshtein_distance\n",
    "from code.modules.model_simplifier_by_rules import simplify_by_rules\n",
    "from code.modules.model_reconstructer import model_reconstruct\n",
    "from code.modules.isomorphism_distance import isomorphism_distance\n",
    "from code.modules.patterns_extracter import extract_patterns\n",
    "from code.modules.patterns_extracter import extract_patterns\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_path = 'populations/collected_models14/'\n",
    "#ts_labels = ['chest_volume', 'heart_rate', 'oxygen_concentration', 'open_apple']\n",
    "ts_labels = sorted(['chest_volume', 'open_apple'])\n",
    "\n",
    "number_ts_pieces = 36\n",
    "tokens_which_param_interest = ['normal_', 'sina_']\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text):    \n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "filenames = sorted(os.listdir(files_path), key=natural_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Collect primitive structural features from a population of models**\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_simple_features_from_segment_of_ts(number_of_file, type_of_ts):\n",
    "    number_of_retrieved_models = 10\n",
    "    \n",
    "    tokens_codes, _ = create_map_tokens_params()\n",
    "    filename = type_of_ts + '_' + str(number_of_file + 1) + '.txt'\n",
    "    \n",
    "    models = get_population_from_file(filename)[0:number_of_retrieved_models]\n",
    "    primitive_frequences = np.zeros(len(tokens_codes) - 1, dtype = float)\n",
    "    lower_bound_code_variables = tokens_codes.get('x0', len(tokens_codes))\n",
    "    \n",
    "    for model in models:\n",
    "        matr, encodings = dfs_search_on_handle(model)\n",
    "        model_primitive_frequences = Counter(encodings)\n",
    "        for key in model_primitive_frequences:\n",
    "            if key >= 0 and key < len(tokens_codes) - 1:\n",
    "                primitive_frequences[key] += (model_primitive_frequences[key] / len(model_primitive_frequences))\n",
    "    return scale(primitive_frequences.reshape(-1,1), axis=0)\n",
    "    \n",
    "    #primitive_frequences[-1] = len(encodings)\n",
    "    #return normalize(primitive_frequences.reshape(-1,1), axis=0)\n",
    "    #return primitive_frequences.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Trivial features creation**\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 18)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_codes, _ = create_map_tokens_params()\n",
    "feature_matrices_of_ts = {label : np.zeros((number_ts_pieces, len(tokens_codes) - 1)) for label in ts_labels}\n",
    "\n",
    "for label in ts_labels:\n",
    "    for index in range(number_ts_pieces):\n",
    "        feature_matrices_of_ts[label][index,:] = get_simple_features_from_segment_of_ts(index, label)[:,0]\n",
    "# feature_matrices_of_ts = {label : scale(feature_matrices_of_ts[label].T) for label in ts_labels}          \n",
    "unite_feature_matrix = np.vstack((feature_matrices_of_ts[label] for label in ts_labels))\n",
    "unite_feature_matrix_backup = unite_feature_matrix\n",
    "unite_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Features on parameters of superpositions **\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 39)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_codes, tokens_params = create_map_tokens_params()\n",
    "total_num_of_params         = sum(tokens_params.values())\n",
    "    \n",
    "feature_matrices_of_ts_ext = {label : np.zeros((number_ts_pieces, total_num_of_params)) for label in ts_labels}\n",
    "\n",
    "for label in ts_labels:\n",
    "    for index in range(number_ts_pieces):\n",
    "        feature_matrices_of_ts_ext[label][index,:] = get_param_features_from_segment_of_ts(index, label)[:,0]\n",
    "unite_feature_matrix_add = np.vstack((feature_matrices_of_ts_ext[label] for label in ts_labels))\n",
    "unite_feature_matrix = np.hstack((unite_feature_matrix_backup, unite_feature_matrix_add)) \n",
    "unite_feature_matrix_param_backup = unite_feature_matrix\n",
    "unite_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 22)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_of_params_interesting_tokens = get_indices_of_params_interesting_tokens(tokens_which_param_interest)\n",
    "unite_feature_matrix_add_new = np.array([unite_feature_matrix_add[:,ind] for ind in indices_of_params_interesting_tokens])\n",
    "unite_feature_matrix_add_new = unite_feature_matrix_add_new.T\n",
    "\n",
    "unite_feature_matrix = np.hstack((unite_feature_matrix_backup, unite_feature_matrix_add_new)) \n",
    "unite_feature_matrix_param_backup = unite_feature_matrix\n",
    "unite_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unite_feature_matrix[2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pattern features **\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 43)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countered_patterns_dict = {label : get_countered_useful_patterns(label,desired_number_of_patterns=20) for label in ts_labels}\n",
    "useful_patterns_dict = {label : sorted(countered_patterns_dict[label].keys()) for label in ts_labels}\n",
    "\n",
    "useful_patterns = [useful_patterns_dict[label] for label in ts_labels]\n",
    "#useful_patterns_list = set.difference(*[set(useful_patterns_dict[label]) for label in ts_labels])\n",
    "useful_patterns_list = set([item for sublist in useful_patterns for item in sublist])\n",
    "#useful_patterns_list = set(useful_patterns_dict[ts_labels[0]]).difference(set(useful_patterns_dict[ts_labels[1]]))\n",
    "#useful_patterns_list = useful_patterns_list.union(set(useful_patterns_dict[ts_labels[1]]).difference(set(useful_patterns_dict[ts_labels[0]])) )\n",
    "feature_matrices_of_ts = {label : np.zeros((number_ts_pieces, len(useful_patterns_list))) for label in ts_labels}\n",
    "\n",
    "for label in ts_labels:\n",
    "    for index in range(number_ts_pieces):\n",
    "        feature_matrices_of_ts[label][index,:] = \\\n",
    "        get_features_patterns_population(get_population_from_file(create_filename(label,index+1)),list(useful_patterns_list))[:,0]\n",
    "#unite_feature_matrix_add = np.vstack((feature_matrices_of_ts_ext[label] for label in ts_labels))\n",
    "unite_feature_matrix_add = np.vstack((feature_matrices_of_ts[label] for label in ts_labels))\n",
    "#unite_feature_matrix = unite_feature_matrix_add\n",
    "unite_feature_matrix = np.hstack((unite_feature_matrix_param_backup, unite_feature_matrix_add)) \n",
    "\n",
    "unite_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unite_feature_matrix[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'lnl_(sin_(lnl_))': 18,\n",
       "          'lnl_(sin_)': 29,\n",
       "          'lnl_(sina_(sin_))': 32,\n",
       "          'lnl_(sina_)': 63,\n",
       "          'plus2_(sin_(x0),sin_(x0))': 17,\n",
       "          'plus2_(sin_(x0),x0)': 29,\n",
       "          'plus2_(sina_(x0),x0)': 60,\n",
       "          'sin_(inv_)': 24,\n",
       "          'sin_(sin_(x0))': 16,\n",
       "          'sin_(sin_)': 31,\n",
       "          'sin_(sina_(sina_))': 38,\n",
       "          'sin_(sina_)': 102,\n",
       "          'sin_(tana_)': 31,\n",
       "          'sina_(sin_)': 83,\n",
       "          'sina_(sina_(sina_))': 27,\n",
       "          'sina_(sina_)': 102,\n",
       "          'sina_(sqrtl_)': 34,\n",
       "          'tana_(sina_)': 28,\n",
       "          'times2_(sina_(x0),x0)': 20}),\n",
       " Counter({'lnl_(sin_(sina_))': 38,\n",
       "          'lnl_(sin_)': 38,\n",
       "          'lnl_(sina_)': 44,\n",
       "          'minus2_(sin_(x0),x0)': 44,\n",
       "          'minus2_(x0,sin_(x0))': 33,\n",
       "          'plus2_(sin_(sin_(x0)),x0)': 19,\n",
       "          'plus2_(sin_(x0),x0)': 44,\n",
       "          'plus2_(sina_(sina_(x0)),x0)': 17,\n",
       "          'plus2_(sina_(x0),x0)': 83,\n",
       "          'sin_(sin_)': 97,\n",
       "          'sin_(sina_)': 132,\n",
       "          'sin_(sqrtl_)': 43,\n",
       "          'sin_(tana_)': 17,\n",
       "          'sina_(sin_)': 30,\n",
       "          'sina_(sina_)': 114,\n",
       "          'sina_(sqrtl_)': 58,\n",
       "          'sqrtl_(sina_)': 24,\n",
       "          'times2_(sin_(x0),x0)': 22,\n",
       "          'times2_(sina_(x0),x0)': 40}))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countered_patterns_dict['heart_rate'], countered_patterns_dict['open_apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  2.,  1.,\n",
       "        0.,  0.,  1.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrices_of_ts['chest_volume'][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_labels = list(feature_matrices_of_ts.keys())\n",
    "ts_labels_in_my_own_order = ['chest_volume', 'heart_rate', 'oxygen_concentration', 'open_apple']\n",
    "which_label_is_positive = ts_labels_in_my_own_order[0]\n",
    "\n",
    "target_vector = np.zeros((unite_feature_matrix.shape[0],1))\n",
    "ts_labels_in_order_from_dictionary = [label for label in feature_matrices_of_ts]\n",
    "index_of_label_positive = ts_labels_in_order_from_dictionary.index(which_label_is_positive)\n",
    "all_inidices_of_samples = np.arange(target_vector.shape[0])\n",
    "positive_indices_samples = number_ts_pieces * index_of_label_positive + np.arange(number_ts_pieces)\n",
    "negative_positive_samples = [ind for ind in all_inidices_of_samples if not ind in positive_indices_samples]\n",
    "target_vector[number_ts_pieces * index_of_label_positive:number_ts_pieces * (index_of_label_positive + 1)] = np.ones((number_ts_pieces,1))\n",
    "backup_target = target_vector\n",
    "\n",
    "fraction_of_test_samples = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error =  0.189253462942\n",
      "error on positive =  0.181492832168\n",
      "error on negative =  0.188921410126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "trials = 500\n",
    "cumulative_error = 0\n",
    "cumulative_error_pos = 0\n",
    "cumulative_error_neg = 0\n",
    "\n",
    "for trying in range(trials):\n",
    "    indices_of_test_sample = np.random.choice([True, False], len(all_inidices_of_samples), p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "    target_vector = 2* backup_target - 1 \n",
    "    \n",
    "    train_matrix = unite_feature_matrix[~indices_of_test_sample,:]\n",
    "    test_matrix = unite_feature_matrix[indices_of_test_sample,:]\n",
    "    train_target = target_vector[~indices_of_test_sample].reshape(sum(~indices_of_test_sample),)\n",
    "    test_target = target_vector[indices_of_test_sample].reshape(sum(indices_of_test_sample),)\n",
    "\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(train_matrix, train_target) \n",
    "    \n",
    "    predictions = clf.predict(test_matrix)\n",
    "    #print(\"predictions: \", predictions)\n",
    "    errors = predictions != test_target\n",
    "    cumulative_error += sum(errors) / len(errors)\n",
    "    cumulative_error_pos += sum(errors[test_target == max(test_target)]) / len(errors[test_target == max(test_target)])\n",
    "    cumulative_error_neg += sum(errors[test_target == min(test_target)]) / len(errors[test_target == min(test_target)])\n",
    "\n",
    "cumulative_error /= trials\n",
    "cumulative_error_pos /= trials\n",
    "cumulative_error_neg /= trials\n",
    "\n",
    "print(\"error = \", cumulative_error)\n",
    "print(\"error on positive = \", cumulative_error_pos)\n",
    "print(\"error on negative = \", cumulative_error_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chest_volume\n",
      "open_apple\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "means_of_frequencies = np.zeros((unite_feature_matrix.shape[1],len(ts_labels)))\n",
    "for ind_label, label in enumerate(ts_labels):\n",
    "    print(label)\n",
    "    matr = feature_matrices_of_ts[label]\n",
    "    for ind_token in range(matr.shape[1]):\n",
    "        means_of_frequencies[ind_token, ind_label] = np.mean(matr[:,ind_token])# / np.sum(np.mean(matr[:,ind_token]))\n",
    "\n",
    "np.savetxt('means.txt', means_of_frequencies, fmt = '%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, True, False, True, False, False, True, False, True, True, False, True, True, False, True, False, False, True, False, False, False, True, True, False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr = np.loadtxt('means.txt')\n",
    "f = matr[:,0]\n",
    "s = matr[:,1]\n",
    "values = np.zeros(number_ts_pieces)\n",
    "pos_ind = [f[i] > s[i] for i in range(f.shape[0])]\n",
    "print(pos_ind)\n",
    "for ind in range(number_ts_pieces):\n",
    "    row =  unite_feature_matrix[number_ts_pieces + ind,:] \n",
    "    \n",
    "    fd = sum([row[i] for i in range(len(pos_ind)) if pos_ind[i]])\n",
    "    sd = sum(row) - fd\n",
    "\n",
    "    if (fd > sd):\n",
    "        values[ind] = 1\n",
    "values        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pos_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27 28]\n",
      "error =  0.5\n",
      "error on positive =  0.375\n",
      "error on negative =  0.571428571429\n"
     ]
    }
   ],
   "source": [
    "indices_of_test_sample = np.random.choice([True, False], len(all_inidices_of_samples), p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "target_vector = 2* backup_target - 1 \n",
    "train_matrix = unite_feature_matrix[~indices_of_test_sample,:]\n",
    "test_matrix = unite_feature_matrix[indices_of_test_sample,:]\n",
    "train_target = target_vector[~indices_of_test_sample].reshape(sum(~indices_of_test_sample),)\n",
    "test_target = target_vector[indices_of_test_sample].reshape(sum(indices_of_test_sample),)\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_matrix, train_target) \n",
    "print(clf.n_support_)\n",
    "predictions = clf.predict(test_matrix)\n",
    "#print(\"predictions: \", predictions)\n",
    "errors = predictions != test_target\n",
    "\n",
    "print(\"error = \", sum(errors) / len(errors))\n",
    "print(\"error on positive = \", sum(errors[test_target == max(test_target)]) / len(errors[test_target == max(test_target)]))\n",
    "print(\"error on negative = \", sum(errors[test_target == min(test_target)]) / len(errors[test_target == min(test_target)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 16 31 34 43 60 66 83 97]\n",
      "[31 60 83]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(len(indices_of_test_sample))[indices_of_test_sample])\n",
    "print(np.arange(len(indices_of_test_sample))[indices_of_test_sample][np.arange(len(errors))[errors]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_unite_feature_matrix = unite_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.44765882695184861)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "error = np.inf\n",
    "where_min = -1\n",
    "trials = 30\n",
    "\n",
    "for n_neighbors in 1 + np.arange(30):\n",
    "    for weights in ['uniform', 'distance']:\n",
    "        cumulative_error = 0\n",
    "        for trying in range(trials):\n",
    "            indices_of_test_sample = np.random.choice([True, False], len(all_inidices_of_samples), p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "            target_vector = 2* backup_target - 1 \n",
    "            train_matrix = unite_feature_matrix[~indices_of_test_sample,:]\n",
    "            test_matrix = unite_feature_matrix[indices_of_test_sample,:]\n",
    "            train_target = target_vector[~indices_of_test_sample].reshape(sum(~indices_of_test_sample),)\n",
    "            test_target = target_vector[indices_of_test_sample].reshape(sum(indices_of_test_sample),)\n",
    "            \n",
    "            # we create an instance of Neighbours Classifier and fit the data.\n",
    "            clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "            clf.fit(train_matrix, train_target)\n",
    "\n",
    "            predictions = clf.predict(np.c_[test_matrix])\n",
    "            errors = predictions != test_target\n",
    "            \n",
    "            cumulative_error = cumulative_error + (sum(errors) / len(errors))\n",
    "        if cumulative_error / trials < error:\n",
    "            where_min = n_neighbors\n",
    "            error = cumulative_error / trials\n",
    "\n",
    "\n",
    "where_min, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Structural metrics (Levenshtein on strings and graphs) **\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance_between_populations_string(population_first, population_second):\n",
    "    len_first, len_second = len(population_first), len(population_second)\n",
    "    # calculate sum of distances for all pairs of models: one from first population, the other from second\n",
    "    cumulative_distance = 0\n",
    "    for model_from_first in population_first:\n",
    "        for model_from_second in population_second:\n",
    "            cumulative_distance = cumulative_distance + levenshtein_distance(model_from_first, model_from_second)\n",
    "    cumulative_distance = cumulative_distance / (len_first * len_second)\n",
    "    return cumulative_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance_between_populations_trees(population_first, population_second):\n",
    "    len_first, len_second = len(population_first), len(population_second)\n",
    "    # calculate sum of distances for all pairs of models: one from first population, the other from second\n",
    "    cumulative_distance = 0\n",
    "    for model_from_first in population_first:\n",
    "        for model_from_second in population_second:\n",
    "            cumulative_distance = cumulative_distance + isomorphism_distance(model_from_first, model_from_second)\n",
    "    cumulative_distance = cumulative_distance / (len_first * len_second)\n",
    "    return cumulative_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_matrix_of_codes_of_one_population(number_of_file, type_of_ts, tokens_codes):\n",
    "    codes = 'QWERTYUIOPASDFGHJKLZXCVBNM123456789/*-+=?!'\n",
    "    filename = type_of_ts + '_' + str(number_of_file + 1) + '.txt'\n",
    "    models = get_population_from_file(filename)\n",
    "    \n",
    "    matrix_representation = []    \n",
    "    for model in models:\n",
    "        matr, encodings = dfs_search_on_handle(model)\n",
    "        encodings = np.array(encodings)\n",
    "        if len(model) == 0:\n",
    "            break\n",
    "        matrix_representation.append(''.join(np.array(list(codes))[encodings]))\n",
    "\n",
    "    return matrix_representation\n",
    "    #return primitive_frequences.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create string-format DFS-code of the final selected models stored in the corresponding files **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_codes, _ = create_map_tokens_params()\n",
    "feature_matrices_of_ts = {label : [] for label in ts_labels}\n",
    "    \n",
    "for label in ts_labels:\n",
    "    for index in range(number_ts_pieces):\n",
    "        feature_matrices_of_ts[label].append(create_matrix_of_codes_of_one_population(index, label, tokens_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 3 ; neg = 7\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n",
      "pos = 4 ; neg = 6\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 7 ; neg = 3\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n",
      "pos = 4 ; neg = 6\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 6 ; neg = 4\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n",
      "pos = 5 ; neg = 5\n",
      "pos = 4 ; neg = 6\n"
     ]
    }
   ],
   "source": [
    "for i in range(180):\n",
    "    row = distances_between_segments[i,:]\n",
    "    inds= np.array([i[0] for i in sorted(enumerate(row), key=lambda x:x[1])])[0:10]\n",
    "    threshhold = number_ts_pieces\n",
    "\n",
    "    pos = sum(inds < number_ts_pieces)\n",
    "    neg = inds.shape[0] - pos\n",
    "    print('pos =', pos, '; neg =', neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isomorphism_distance('minus2_(linear_(sina_(sina_(x0))),normal_(normal_(x0)))', 'minus2_(linear_(sina_(sina_(x0))),normal_(normal_(x0)))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minus2_(linear_(sina_(sina_(x0))),normal_(normal_(x0))) minus2_(linear_(sina_(sina_(x0))),normal_(normal_(x0)))\n",
      "771.6964039802551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_between_segments = np.zeros((len(ts_labels) * number_ts_pieces, len(ts_labels) * number_ts_pieces))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "indices_vs_labels = list(enumerate(feature_matrices_of_ts))\n",
    "\n",
    "for ind_f in range(distances_between_segments.shape[0]):\n",
    "    for ind_s in range(distances_between_segments.shape[0]):\n",
    "        label_f = indices_vs_labels[ind_f // number_ts_pieces][1]\n",
    "        label_s = indices_vs_labels[ind_s // number_ts_pieces][1]\n",
    "        \n",
    "        #population_f = feature_matrices_of_ts[label_f][ind_f % number_ts_pieces]\n",
    "        #population_s = feature_matrices_of_ts[label_s][ind_s % number_ts_pieces]\n",
    "        population_f = get_population_from_file(label_f + '_' + str(ind_f % number_ts_pieces + 1) + '.txt')\n",
    "        population_s = get_population_from_file(label_s + '_' + str(ind_s % number_ts_pieces + 1) + '.txt')\n",
    "        \n",
    "        if ind_f <= ind_s:\n",
    "            if ind_f == 0 and ind_s == 0:\n",
    "                print(population_f[0], population_s[0])\n",
    "            distances_between_segments[ind_f][ind_s] = distance_between_populations_trees(population_f[0:10], population_s[0:10])\n",
    "        else:\n",
    "            distances_between_segments[ind_f][ind_s] = distances_between_segments[ind_s][ind_f]\n",
    "            \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % ( 1, 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error =  0.494774985909\n"
     ]
    }
   ],
   "source": [
    "trials = 200\n",
    "measurements = np.zeros(trials)\n",
    "for ii in range(trials):\n",
    "    fraction_of_test_samples = 0.3\n",
    "\n",
    "    indices_of_test_sample = np.random.choice([True, False], number_ts_pieces, p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "    indices_of_test_sample = np.arange(number_ts_pieces)[indices_of_test_sample]\n",
    "    indices_to_search = np.setdiff1d(range(number_ts_pieces), indices_of_test_sample)\n",
    "    #indices_to_search = np.hstack((indices_to_search, indices_to_search + number_ts_pieces * np.ones(indices_to_search.shape)\n",
    "    #                               , indices_to_search + 2 * number_ts_pieces * np.ones(indices_to_search.shape)))\n",
    "    indices_to_search = np.hstack((indices_to_search, indices_to_search + number_ts_pieces * np.ones(indices_to_search.shape)))\n",
    "\n",
    "    responses = np.empty((len(ts_labels) * len(indices_of_test_sample), 1))\n",
    "\n",
    "    indices_to_search = np.array(indices_to_search, dtype = int)\n",
    "    for ind_label in range(len(ts_labels)):\n",
    "        for index_of_index, index in enumerate(indices_of_test_sample):\n",
    "            index_in_matrix  = ind_label * number_ts_pieces + index\n",
    "            row_for_analysis = distances_between_segments[index_in_matrix,:] \n",
    "\n",
    "            #nearest_neighbor_label = int(find_closest_elem(row_for_analysis, indices_to_search) // number_ts_pieces)\n",
    "            nearest_neighbor_label = indices_to_search[np.argmin(row_for_analysis[indices_to_search])] // number_ts_pieces        \n",
    "            responses[len(indices_of_test_sample) * ind_label + index_of_index] = nearest_neighbor_label\n",
    "\n",
    "    true_responses = np.zeros((len(ts_labels) * len(indices_of_test_sample), 1))\n",
    "    for i in range(len(ts_labels)):\n",
    "        true_responses[i*len(indices_of_test_sample):(i+1)*len(indices_of_test_sample)] = i * np.ones((len(indices_of_test_sample), 1))\n",
    "    measurements[ii] = sum(responses != true_responses) / len(true_responses)\n",
    "    #print(\"error = \", sum(responses != true_responses) / len(true_responses))\n",
    "print(\"error = \", np.mean(measurements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraction_of_test_samples = 0.3\n",
    "\n",
    "indices_of_test_sample = np.random.choice([True, False], number_ts_pieces, p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "indices_of_test_sample = np.arange(number_ts_pieces)[indices_of_test_sample]\n",
    "indices_to_search = np.setdiff1d(range(number_ts_pieces), indices_of_test_sample)\n",
    "#indices_to_search = np.hstack((indices_to_search, indices_to_search + number_ts_pieces * np.ones(indices_to_search.shape)\n",
    "#                               , indices_to_search + 2 * number_ts_pieces * np.ones(indices_to_search.shape)))\n",
    "indices_to_search = np.hstack((indices_to_search, indices_to_search + number_ts_pieces * np.ones(indices_to_search.shape)))\n",
    "\n",
    "responses = np.empty((len(ts_labels) * len(indices_of_test_sample), 1))\n",
    "\n",
    "indices_to_search = np.array(indices_to_search, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.41666667])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ind_label in range(len(ts_labels)):\n",
    "    for index_of_index, index in enumerate(indices_of_test_sample):\n",
    "        index_in_matrix  = ind_label * number_ts_pieces + index\n",
    "        row_for_analysis = distances_between_segments[index_in_matrix,:] \n",
    "\n",
    "        #nearest_neighbor_label = int(find_closest_elem(row_for_analysis, indices_to_search) // number_ts_pieces)\n",
    "        nearest_neighbor_label = indices_to_search[np.argmin(row_for_analysis[indices_to_search])] // number_ts_pieces        \n",
    "        responses[len(indices_of_test_sample) * ind_label + index_of_index] = nearest_neighbor_label\n",
    "\n",
    "true_responses = np.zeros((len(ts_labels) * len(indices_of_test_sample), 1))\n",
    "for i in range(len(ts_labels)):\n",
    "    true_responses[i*len(indices_of_test_sample):(i+1)*len(indices_of_test_sample)] = i * np.ones((len(indices_of_test_sample), 1))\n",
    "sum(responses != true_responses) / len(true_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_in_matrix  = 0 * number_ts_pieces + 2\n",
    "row_for_analysis = distances_between_segments[0,:] \n",
    "\n",
    "nearest_neighbor_label = indices_to_search[np.argmin(row_for_analysis[indices_to_search])] // number_ts_pieces        \n",
    "responses[len(indices_of_test_sample) * ind_label + index_of_index] = nearest_neighbor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_closest_elem(row_for_analysis, indices_to_search):\n",
    "    minim = np.Inf\n",
    "    posit = -1\n",
    "    for ind in indices_to_search:\n",
    "        if row_for_analysis[ind] < minim:\n",
    "            posit = ind\n",
    "            minim = row_for_analysis[ind]\n",
    "    return posit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,  15.,  12.,  11.,  11.],\n",
       "       [ 15.,   0.,  14.,  12.,   8.],\n",
       "       [ 12.,  14.,   0.,  11.,  10.],\n",
       "       [ 11.,  12.,  11.,   0.,   7.],\n",
       "       [ 11.,   8.,  10.,   7.,   0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_between_segments[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96040539,  1.        ],\n",
       "       [ 1.        ,  0.98376707]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_mat = np.empty((len(ts_labels),len(ts_labels)))\n",
    "length_side = distances_between_segments.shape[0] // 3\n",
    "for i in range(len(ts_labels)):\n",
    "    for j in range(len(ts_labels)):\n",
    "        ul = i * length_side\n",
    "        ur = ul + length_side\n",
    "        dl = j * length_side\n",
    "        dr = dl + length_side\n",
    "        for k in np.arange(ul, ur, 1):\n",
    "            for q in np.arange(dl, dr, 1):\n",
    "                blocks_mat[i,j] += distances_between_segments[k,q]\n",
    "\n",
    "blocks_mat  / np.max(blocks_mat.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraction_of_test_samples = 0.3\n",
    "\n",
    "indices_of_test_sample = np.random.choice([True, False], number_ts_pieces, p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "indices_of_test_sample = np.arange(number_ts_pieces)[indices_of_test_sample]\n",
    "indices_to_search = np.setdiff1d(range(number_ts_pieces), indices_of_test_sample)\n",
    "#indices_to_search = np.hstack((indices_to_search, indices_to_search + number_ts_pieces * np.ones(indices_to_search.shape)\n",
    "#                               , indices_to_search + 2 * number_ts_pieces * np.ones(indices_to_search.shape)))\n",
    "indices_to_search = np.hstack((indices_to_search, indices_to_search + number_ts_pieces * np.ones(indices_to_search.shape)))\n",
    "\n",
    "responses = np.empty((len(ts_labels) * len(indices_of_test_sample), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169, 22, 262, 157, 65]\n",
      "[ 8.70222222  9.01777778  9.15555556  9.32        9.32444444]\n"
     ]
    }
   ],
   "source": [
    "a = distances_between_segments[10,:]\n",
    "order_neighbors = sorted(indices_to_search, key = lambda x: a[x])\n",
    "print(order_neighbors[0:5])\n",
    "print(a[order_neighbors[0:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound = 1\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 3\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 5\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 7\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 9\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 11\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 13\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 15\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 17\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 19\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 21\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 23\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 25\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 27\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 29\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 31\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 33\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 35\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 37\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 39\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 41\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "bound = 43\n",
      "JXBJVBEBDXEJEBFBEB JXBJVBEBDXEJEBFBEB\n",
      "min_error = 0.0952263118095 ;\n",
      "which_min = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = np.arange(1,45,step=2)\n",
    "min_error = np.Inf\n",
    "which_min = -1\n",
    "\n",
    "for bound in bounds:\n",
    "    print(\"bound =\",bound)\n",
    "    distances_between_segments = np.zeros((len(ts_labels) * number_ts_pieces, len(ts_labels) * number_ts_pieces))\n",
    "\n",
    "    indices_vs_labels = list(enumerate(feature_matrices_of_ts))\n",
    "\n",
    "    for ind_f in range(distances_between_segments.shape[0]):\n",
    "        for ind_s in range(distances_between_segments.shape[0]):\n",
    "            if ind_f % 20 == 0 and ind_s % 20 == 0:\n",
    "                #print(ind_f, ind_s)\n",
    "                pass\n",
    "            label_f = indices_vs_labels[ind_f // number_ts_pieces][1]\n",
    "            label_s = indices_vs_labels[ind_s // number_ts_pieces][1]\n",
    "\n",
    "            population_f = feature_matrices_of_ts[label_f][ind_f % number_ts_pieces]\n",
    "            population_s = feature_matrices_of_ts[label_s][ind_s % number_ts_pieces]\n",
    "\n",
    "            if ind_f <= ind_s:\n",
    "                if ind_f == 0 and ind_s == 0:\n",
    "                    print(population_f[0], population_s[0])\n",
    "                distances_between_segments[ind_f][ind_s] = distance_between_populations(population_f[0:bound], population_s[0:bound])\n",
    "            else:\n",
    "                distances_between_segments[ind_f][ind_s] = distances_between_segments[ind_s][ind_f]\n",
    "\n",
    "\n",
    "    trials = 200\n",
    "    measurements = np.zeros(trials)\n",
    "    for ii in range(trials):\n",
    "        fraction_of_test_samples = 0.3\n",
    "\n",
    "        indices_of_test_sample = np.random.choice([True, False], number_ts_pieces, p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "        indices_of_test_sample = np.arange(number_ts_pieces)[indices_of_test_sample]\n",
    "        indices_to_search = np.setdiff1d(range(number_ts_pieces), indices_of_test_sample)\n",
    "        #indices_to_search = np.hstack((indices_to_search, indices_to_search + number_ts_pieces * np.ones(indices_to_search.shape)\n",
    "        #                               , indices_to_search + 2 * number_ts_pieces * np.ones(indices_to_search.shape)))\n",
    "        indices_to_search = np.hstack((indices_to_search, indices_to_search + number_ts_pieces * np.ones(indices_to_search.shape)))\n",
    "\n",
    "        responses = np.empty((len(ts_labels) * len(indices_of_test_sample), 1))\n",
    "\n",
    "        indices_to_search = np.array(indices_to_search, dtype = int)\n",
    "        for ind_label in range(len(ts_labels)):\n",
    "            for index_of_index, index in enumerate(indices_of_test_sample):\n",
    "                index_in_matrix  = ind_label * number_ts_pieces + index\n",
    "                row_for_analysis = distances_between_segments[index_in_matrix,:] \n",
    "\n",
    "                #nearest_neighbor_label = int(find_closest_elem(row_for_analysis, indices_to_search) // number_ts_pieces)\n",
    "                nearest_neighbor_label = indices_to_search[np.argmin(row_for_analysis[indices_to_search])] // number_ts_pieces        \n",
    "                responses[len(indices_of_test_sample) * ind_label + index_of_index] = nearest_neighbor_label\n",
    "\n",
    "        true_responses = np.zeros((len(ts_labels) * len(indices_of_test_sample), 1))\n",
    "        true_responses[len(indices_of_test_sample):2 * len(indices_of_test_sample)] = np.ones((len(indices_of_test_sample), 1))\n",
    "        #true_responses[2 * len(indices_of_test_sample):3 * len(indices_of_test_sample)] = 2 * np.ones((len(indices_of_test_sample), 1))\n",
    "        measurements[ii] = sum(responses != true_responses) / len(true_responses)\n",
    "        #print(\"error = \", sum(responses != true_responses) / len(true_responses))\n",
    "    if np.mean(measurements) < min_error:\n",
    "        min_error = np.mean(measurements)\n",
    "        which_min = bound\n",
    "        \n",
    "print(\"min_error =\", min_error,\";\\nwhich_min =\", which_min)        \n",
    "os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % ( 1, 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "0.5323779582977295\n"
     ]
    }
   ],
   "source": [
    "population_f = feature_matrices_of_ts['chest_volume']\n",
    "population_s = feature_matrices_of_ts['heart_rate']\n",
    "print(len(population_f))\n",
    "import time\n",
    "start = time.time()\n",
    "distance_between_populations(population_f[0], population_s[0])\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.615384615385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 50\n",
    "clusters = KMeans(n_clusters=len(ts_labels), random_state=random_state,n_init = 10).fit_predict(distances_between_segments)\n",
    "true_clusters = np.zeros(len(ts_labels) * number_ts_pieces)\n",
    "for ind,_ in enumerate(ts_labels):\n",
    "    true_clusters[ind * number_ts_pieces:(ind + 1) * number_ts_pieces] = ind * np.ones(number_ts_pieces)\n",
    "print(sum(clusters != true_clusters) / len(clusters))\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.371794871795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 170\n",
    "clusters = AgglomerativeClustering(n_clusters=len(ts_labels)).fit_predict(distances_between_segments)\n",
    "print(sum(clusters != true_clusters) / len(clusters))\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraction_of_test_samples = 0.3\n",
    "\n",
    "test_indices = np.random.choice([True, False], number_ts_pieces, p = [fraction_of_test_samples, 1 - fraction_of_test_samples])\n",
    "target_vector = 2* target_vector - 1 \n",
    "\n",
    "train_matrix = unite_feature_matrix[~indices_of_test_sample,:]\n",
    "test_matrix = unite_feature_matrix[indices_of_test_sample,:]\n",
    "train_target = target_vector[~indices_of_test_sample].reshape(sum(~indices_of_test_sample),)\n",
    "test_target = target_vector[indices_of_test_sample].reshape(sum(indices_of_test_sample),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** APPENDIX **\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_population_from_file(filename):\n",
    "    if not filename in filenames:\n",
    "        print('Error:', filename, 'is not presented in the directory')\n",
    "    \n",
    "    lines_file_content = []\n",
    "    \n",
    "    with open(files_path + filename, 'r') as f_in:\n",
    "        lines_file_content = (line.rstrip() for line in f_in) # All lines including the blank ones\n",
    "        lines_file_content = [line for line in lines_file_content if line] # Non-blank lines\n",
    "    #    #population = np.empty(len(lines_file_content) // 2, dtype = object)\n",
    "    population = np.empty(len(lines_file_content) // 2, dtype = object)\n",
    "    \n",
    "    \"\"\"for ind, entity in enumerate(lines_file_content):          \n",
    "        model_name = entity.split(' ')[-1]\n",
    "        population[ind] = re.sub(r'X\\[(\\d+)\\]', r'x\\1', model_name.strip())\"\"\"\n",
    "    for ind, entity in enumerate(lines_file_content):          \n",
    "        if ind % 2 == 0:\n",
    "            model_name = entity.split(' ')[-1]\n",
    "            population[ind // 2] = re.sub(r'X\\[(\\d+)\\]', r'x\\1', model_name.strip())\n",
    "                                              \n",
    "                                          \n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_num_vars_and_tokens(handle):    \n",
    "    counter_tokens = 0\n",
    "    counter_variables = 0\n",
    "\n",
    "    for i in range(len(handle)):\n",
    "        if handle[i] == '_': \n",
    "            counter_tokens += 1\n",
    "        elif i < len(handle)-1 and handle[i] == 'x' and handle[i+1].isdigit():\n",
    "            counter_variables += 1;\n",
    "\n",
    "    return (counter_tokens, counter_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_map_tokens_params():\n",
    "    file_opened = open('data/tokensInterest.txt', 'r')\n",
    "    primitives_lines = file_opened.readlines()\n",
    "    tokens_codes = {line.split()[0] : int(ind) for ind,line in enumerate(primitives_lines)}    \n",
    "    tokens_params = {line.split()[0] : int(line.split()[1]) for line in primitives_lines}    \n",
    "    return (tokens_codes, tokens_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfs_search_on_handle(handle):\n",
    "    counters = find_num_vars_and_tokens(handle)\n",
    "    number_tokens = counters[0] + counters[1]\n",
    "    \n",
    "    waiting_tokens = []\n",
    "    encodings = np.zeros(number_tokens, dtype = int)        \n",
    "    current_token, left, right = 0, 0, 0\n",
    "    is_a_token_processed_now = False    \n",
    "    \n",
    "    map_tokens_params = create_map_tokens_params()[0]\n",
    "    \n",
    "    for right in range(len(handle)):\n",
    "        if handle[right] == '_':\n",
    "            # the root is detected\n",
    "            waiting_tokens.append(current_token)\n",
    "            token = handle[left:right + 1]\n",
    "            encodings[current_token] = map_tokens_params.get(token, -1)\n",
    "            right += 1\n",
    "            break;  \n",
    "    \n",
    "    matr = [[] for i in range(number_tokens)]            \n",
    "    \n",
    "    # now process the remaining vertices\n",
    "    reserved_right = right\n",
    "    for right in np.arange(right, len(handle)):\n",
    "        if handle[right] == ')':\n",
    "            waiting_tokens.pop()        \n",
    "    \n",
    "        if not is_a_token_processed_now and handle[right].isalpha():\n",
    "            is_a_token_processed_now = True\n",
    "            left = right\n",
    "    \n",
    "        # if a token is found\n",
    "        if handle[right] == '_':\n",
    "            # new token is detected\n",
    "            current_token += 1\n",
    "            matr[waiting_tokens[-1]].append(current_token)\n",
    "            waiting_tokens.append(current_token)\n",
    "            token = handle[left:right + 1]\n",
    "            encodings[current_token] = map_tokens_params.get(token, -1)\n",
    "            is_a_token_processed_now = False      \n",
    "        \n",
    "        # if a variable is found\n",
    "        if right < len(handle)-1 and handle[right] == 'x' and handle[right+1].isdigit():\n",
    "            # new variable is detected\n",
    "            current_token += 1\n",
    "            matr[waiting_tokens[-1]].append(current_token)\n",
    "            while right < len(handle)-1 and handle[right] == 'x' and handle[right+1].isdigit():\n",
    "                right += 1\n",
    "            token = handle[left:right + 1]\n",
    "            encodings[current_token] = map_tokens_params.get(token, -1)\n",
    "            is_a_token_processed_now = False            \n",
    "    \n",
    "    return (matr, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def incidence_to_adjacency(incidence):\n",
    "    size_of_mat = len(incidence)\n",
    "    adjacency = np.zeros((size_of_mat, size_of_mat))    \n",
    "    for ind, row in enumerate(incidence):\n",
    "        adjacency[ind][row] = 1\n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create features on parameters of nonlinear functions **\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parameters_from_file(filename):\n",
    "    if not filename in filenames:\n",
    "        print('Error:', filename, 'is not presented in the directory')\n",
    "    \n",
    "    lines_file_content = []\n",
    "    \n",
    "    with open(files_path + filename, 'r') as f_in:\n",
    "        lines_file_content = (line.rstrip() for line in f_in) # All lines including the blank ones\n",
    "        lines_file_content = [line for line in lines_file_content if line] # Non-blank lines\n",
    "    \n",
    "    array_of_parameters = []\n",
    "    \n",
    "    for ind, entity in enumerate(lines_file_content):          \n",
    "        if ind % 2 == 1:\n",
    "            list_of_parameters = list(map(float,entity.split(', ')))\n",
    "            array_of_parameters.append(np.array(list_of_parameters))\n",
    "    return array_of_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ordered_list_tokens():\n",
    "    file_opened = open('data/tokensInterest.txt', 'r')\n",
    "    primitives_lines = file_opened.readlines()\n",
    "    tokens_names = [line.split()[0] for line in primitives_lines]\n",
    "    return tokens_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_array_ranges_parameters(array_param_nums):\n",
    "    array_ranges = [[0,array_param_nums[0]],]    \n",
    "    for ind, param_num in enumerate(array_param_nums):\n",
    "        if ind == 0:\n",
    "            continue\n",
    "        array_ranges.append([array_ranges[-1][-1],array_ranges[-1][-1]+param_num])\n",
    "    return array_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_bsxfun_to_params(model, parameters):\n",
    "    tokens_codes, tokens_params = create_map_tokens_params()\n",
    "    tokens_names                  = get_ordered_list_tokens()\n",
    "    matr, encodings               = dfs_search_on_handle(model)\n",
    "\n",
    "    model_tokens     = [tokens_names[i] for i in encodings]\n",
    "    array_param_nums = [tokens_params[tokens_names[i]] for i in encodings]\n",
    "    \n",
    "    array_ranges_parameters = get_array_ranges_parameters(array_param_nums)\n",
    "    bsxfun_to_params = {token : np.zeros(tokens_params[token]) for ind, token in enumerate(tokens_names)}\n",
    "    for ind, token in enumerate(model_tokens):\n",
    "        bsxfun_to_params[token] += parameters[array_ranges_parameters[ind][0]:array_ranges_parameters[ind][1]]\n",
    "    return bsxfun_to_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_param_features_from_segment_of_ts(number_of_file, type_of_ts):\n",
    "    number_of_retrieved_models = 10\n",
    "    \n",
    "    tokens_codes, tokens_params = create_map_tokens_params()\n",
    "    tokens_names                = get_ordered_list_tokens()\n",
    "    total_num_of_params         = sum(tokens_params.values())\n",
    "    \n",
    "    filename = type_of_ts + '_' + str(number_of_file + 1) + '.txt'\n",
    "    \n",
    "    \n",
    "    array_of_parameters = get_parameters_from_file(filename)[0:number_of_retrieved_models]\n",
    "    population          = get_population_from_file(filename)[0:number_of_retrieved_models]\n",
    "    \n",
    "    cumulated_params = np.zeros(total_num_of_params)\n",
    "    def sigmoid_array(x):                                        \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    for model, parameters in zip(population, array_of_parameters):\n",
    "        bsxfun_to_params    = do_bsxfun_to_params(model, parameters)\n",
    "        row = np.array([param for token in tokens_names for param in bsxfun_to_params[token]])\n",
    "        cumulated_params   += row\n",
    "    \n",
    "    #return scale(cumulated_params.reshape(-1,1), axis=0)\n",
    "    return scale(cumulated_params.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.40018362],\n",
       "       [-0.41487209],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.02795842],\n",
       "       [ 3.9262091 ],\n",
       "       [ 0.42325242],\n",
       "       [ 2.88716793],\n",
       "       [ 1.46072947],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [ 0.90030533],\n",
       "       [ 0.28292565],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ],\n",
       "       [-0.4107989 ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_param_features_from_segment_of_ts(41, 'heart_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_indices_of_params_interesting_tokens(interesting_tokens):\n",
    "    tokens_codes, tokens_params = create_map_tokens_params()\n",
    "    tokens_names                = get_ordered_list_tokens()\n",
    "    total_num_of_params         = sum(tokens_params.values())\n",
    "\n",
    "    array_param_nums = [tokens_params[token] for token in tokens_names]\n",
    "\n",
    "    array_ranges_parameters = get_array_ranges_parameters(array_param_nums)\n",
    "\n",
    "    array_params_interest = []\n",
    "    for token in tokens_which_param_interest:\n",
    "        range_params = array_ranges_parameters[tokens_names.index(token)]\n",
    "        array_params_interest.extend(list(np.arange(range_params[0], range_params[1], 1)))\n",
    "\n",
    "    return array_params_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Functionality for retrieving nontrivial patterns from population's models **\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(['str', 'str', 'str2'])['str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simplify_model(handle):\n",
    "    handle = re.sub(r'X\\[(\\d+)\\]', r'x\\1', handle)\n",
    "    handle = model_reconstruct(handle)\n",
    "    handle = simplify_by_rules(handle)\n",
    "\n",
    "    handle = re.sub(r'x(\\d+)', r'X[\\1]', handle)\n",
    "    return handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-00ce02b6444c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpopulation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_population_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'heart_rate_1.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msimplified_population\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msimplified_population\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimplify_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7ac17833b9c0>\u001b[0m in \u001b[0;36mget_population_from_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_population_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Error:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'is not presented in the directory'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlines_file_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filenames' is not defined"
     ]
    }
   ],
   "source": [
    "population = get_population_from_file('heart_rate_1.txt')\n",
    "simplified_population = []\n",
    "for model in population:\n",
    "    simplified_population.append(simplify_model(model))\n",
    "for ind, model in enumerate(population):\n",
    "    print(model, '\\n-->', simplified_population[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isomorphism_distance('plus_(sina_(normal_(X[0])))', 'X[0]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Extract and work with patterns of models from the stored populations ** \n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_patterns_from_model(model_handle):\n",
    "    unite_string_with_patterns = extract_patterns(model_handle)\n",
    "    patterns = unite_string_with_patterns.split('&')\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_patterns_from_population(population):\n",
    "    patterns = []\n",
    "    for model in population:\n",
    "        model = model_reconstruct(model)\n",
    "        patterns.extend(extract_patterns_from_model(model))\n",
    "    patterns = [item for item in patterns if item]\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_patterns_from_label(label):\n",
    "    patterns = []\n",
    "    for i in np.arange(1, number_ts_pieces, 1):\n",
    "        patterns.extend(collect_patterns_from_population(get_population_from_file(label+'_'+str(i)+'.txt')))\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_popular_patterns(label, threshhold_popularity = 20):\n",
    "    \n",
    "    patterns = collect_patterns_from_label(label)\n",
    "    countered_patterns = Counter(patterns)\n",
    "\n",
    "    popular_patterns = [item for item in set(patterns) if countered_patterns[item] >= threshhold_popularity]\n",
    "    \n",
    "    return popular_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_threshhold_for_patterns_selection(label, desired_number_of_patterns):\n",
    "    \n",
    "    threshhold_popularity = 0\n",
    "\n",
    "    patterns = collect_patterns_from_label(label)\n",
    "    unique_patterns = set(patterns)\n",
    "    countered_patterns = Counter(patterns)\n",
    "\n",
    "    for threshhold_popularity in np.arange(1,1000,1):\n",
    "        popular_patterns = [item for item in unique_patterns if countered_patterns[item] >= threshhold_popularity]\n",
    "        if len(popular_patterns) <= desired_number_of_patterns:\n",
    "            break\n",
    "    return threshhold_popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_countered_useful_patterns(label,desired_number_of_patterns = 40):\n",
    "    \n",
    "    threshhold_popularity = get_threshhold_for_patterns_selection(label,desired_number_of_patterns)\n",
    "    unique_popular_patterns = get_popular_patterns(label, threshhold_popularity)\n",
    "\n",
    "    all_patterns = collect_patterns_from_label(label)\n",
    "    popular_counter_to_count = [item for item in all_patterns if item in unique_popular_patterns]\n",
    "    return Counter(popular_counter_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_patterns_population(population, useful_patterns_list):\n",
    "    patterns = collect_patterns_from_population(population)\n",
    "    countered_patterns = Counter(patterns)\n",
    "    \n",
    "    features = [countered_patterns.get(item,0) for item in useful_patterns_list]\n",
    "    return scale(np.array(features,dtype=float).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_filename(label, number_of_segment):\n",
    "    return label + '_' + str(number_of_segment) + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'number_ts_pieces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-976c1e4c5a33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcounter_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_countered_useful_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'open_apple'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdesired_number_of_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcounter_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_countered_useful_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chest_volume'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdesired_number_of_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msetlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-45385c4b8ee9>\u001b[0m in \u001b[0;36mget_countered_useful_patterns\u001b[1;34m(label, desired_number_of_patterns)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_countered_useful_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdesired_number_of_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mthreshhold_popularity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_threshhold_for_patterns_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdesired_number_of_patterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0munique_popular_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_popular_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshhold_popularity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-661031eb610f>\u001b[0m in \u001b[0;36mget_threshhold_for_patterns_selection\u001b[1;34m(label, desired_number_of_patterns)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mthreshhold_popularity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpatterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollect_patterns_from_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0munique_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcountered_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-bb891144e65d>\u001b[0m in \u001b[0;36mcollect_patterns_from_label\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcollect_patterns_from_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpatterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_ts_pieces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mpatterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollect_patterns_from_population\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_population_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'number_ts_pieces' is not defined"
     ]
    }
   ],
   "source": [
    "counter_f = get_countered_useful_patterns('open_apple',desired_number_of_patterns = 15)\n",
    "counter_s = get_countered_useful_patterns('chest_volume',desired_number_of_patterns = 15)\n",
    "print(setlist(counter_f.keys()), list(counter_s.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'lnl_(normal_(lnl_(sina_)))': 2,\n",
       "          'lnl_(normal_(lnl_))': 2,\n",
       "          'lnl_(normal_(normal_(atana_)))': 3,\n",
       "          'lnl_(normal_(normal_))': 2,\n",
       "          'lnl_(normal_)': 5,\n",
       "          'normal_(expl_(x0))': 2,\n",
       "          'normal_(normal_(lnl_(inv_)))': 2,\n",
       "          'normal_(normal_)': 3,\n",
       "          'sina_(normal_)': 3}),\n",
       " Counter({'atana_(normal_)': 9,\n",
       "          'expl_(atana_)': 3,\n",
       "          'lnl_(normal_)': 11,\n",
       "          'neg_(sina_)': 8,\n",
       "          'normal_(atana_)': 3,\n",
       "          'normal_(normal_(normal_))': 5,\n",
       "          'normal_(normal_)': 8,\n",
       "          'normal_(tana_)': 11,\n",
       "          'plus2_(normal_(x0),x0)': 7,\n",
       "          'sina_(atana_)': 3,\n",
       "          'sina_(sina_)': 3,\n",
       "          'sqrtl_(normal_)': 6,\n",
       "          'tana_(normal_)': 9,\n",
       "          'times2_(normal_(x0),x0)': 3}))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_countered_useful_patterns('chest_volume',desired_number_of_patterns = 15),get_countered_useful_patterns('open_apple',desired_number_of_patterns = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_patterns_list = \n",
    "patterns = collect_patterns_from_population(population)\n",
    "countered_patterns = Counter(patterns)\n",
    "\n",
    "features = [countered_patterns.get(item,0) for item in useful_patterns_list]\n",
    "return scale(np.array(features,dtype=float).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sin_(tana_(tana_))']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_patterns_from_population(get_population_from_file('heart_rate_1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lnl_(normal_(linear_(sina_(sina_(sina_(x0))))))',\n",
       "       'plus_(normal_(lnl_(linear_(sina_(x0)))))',\n",
       "       'lnl_(normal_(lnl_(normal_(linear_(sina_(x0))))))',\n",
       "       'lnl_(plus_(sina_(sqrtl_(sqrtl_(linear_(sina_(x0)))))))',\n",
       "       'lnl_(lnl_(linear_(sina_(sina_(x0)))))',\n",
       "       'lnl_(plus_(sina_(lnl_(normal_(sina_(x0))))))',\n",
       "       'lnl_(plus_(normal_(sina_(x0))))', 'sina_(linear_(linear_(x0)))',\n",
       "       'plus_(sina_(normal_(x0)))', 'linear_(sina_(sina_(x0)))',\n",
       "       'lnl_(normal_(normal_(x0)))', 'lnl_(normal_(sina_(x0)))',\n",
       "       'lnl_(sina_(normal_(x0)))', 'lnl_(sina_(x0))', 'lnl_(normal_(x0))',\n",
       "       'lnl_(inv_(sina_(x0)))', 'sina_(normal_(lnl_(x0)))',\n",
       "       'inv_(tana_(sina_(sina_(normal_(lnl_(x0))))))',\n",
       "       'plus_(linear_(sina_(lnl_(x0))))',\n",
       "       'sina_(linear_(normal_(sina_(x0))))',\n",
       "       'times2_(x0,normal_(tana_(tana_(x0))))',\n",
       "       'plus_(linear_(sina_(linear_(normal_(sina_(x0))))))',\n",
       "       'linear_(sina_(x0))', 'lnl_(sina_(sqrtl_(sqrtl_(x0))))',\n",
       "       'lnl_(sqrtl_(tana_(x0)))', 'plus_(sina_(x0))',\n",
       "       'plus_(normal_(lnl_(sina_(x0))))',\n",
       "       'inv_(tana_(sina_(normal_(x0))))',\n",
       "       'plus_(normal_(plus_(normal_(x0))))',\n",
       "       'inv_(tana_(sina_(sina_(x0))))',\n",
       "       'plus_(linear_(normal_(sina_(x0))))',\n",
       "       'plus_(lnl_(normal_(linear_(sina_(x0)))))',\n",
       "       'plus_(plus_(sina_(normal_(x0))))',\n",
       "       'plus_(sina_(plus_(sina_(x0))))', 'lnl_(sina_(plus_(sina_(x0))))',\n",
       "       'plus_(sina_(sqrtl_(x0)))', 'lnl_(normal_(lnl_(x0)))',\n",
       "       'lnl_(lnl_(linear_(sina_(x0))))',\n",
       "       'lnl_(normal_(linear_(sina_(x0))))',\n",
       "       'lnl_(normal_(plus_(sina_(x0))))', 'lnl_(sina_(linear_(x0)))',\n",
       "       'lnl_(plus_(sina_(x0)))', 'lnl_(sina_(plus_(x0)))',\n",
       "       'lnl_(linear_(sina_(x0)))', 'lnl_(normal_(linear_(x0)))'], dtype=object)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_population_from_file('heart_rate_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Analysis of distributions of features **\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_token_frequency_in_population(number_of_segment,label):\n",
    "    filename = files_path + label + '_' + str(number_of_file + 1) + '.txt'\n",
    "    population = get_population_from_file(filename)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_names = get_ordered_list_tokens()\n",
    "number_of_tokens = len(tokens_names)\n",
    "for number_of_plot in range(number_of_tokens):\n",
    "    plt.figure(number_of_plot + 1)\n",
    "    plt.hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
